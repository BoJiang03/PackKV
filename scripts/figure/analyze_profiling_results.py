#!/usr/bin/env python3
"""
åˆ†æè„šæœ¬ç”¨äºè§£æprofilingç»“æœå¹¶ç”Ÿæˆå¯è¯»æŠ¥å‘Š
è¯¥è„šæœ¬å¸®åŠ©ç†è§£CUDA kernelä¸PyTorchæ“ä½œçš„å¯¹åº”å…³ç³»
"""

import json
import os
import re
from pathlib import Path
from typing import Dict, List, Tuple
import pandas as pd

def parse_kernel_name(kernel_name: str) -> Dict[str, str]:
    """
    è§£æCUDA kernelåç§°ï¼Œæå–æœ‰ç”¨ä¿¡æ¯
    """
    info = {
        'original_name': kernel_name,
        'operation_type': 'unknown',
        'data_type': 'unknown',
        'kernel_type': 'unknown'
    }
    
    # æ£€æµ‹GEMV/GEMMæ“ä½œ
    if 'gemv' in kernel_name.lower():
        info['operation_type'] = 'GEMV (Matrix-Vector Multiplication)'
        info['kernel_type'] = 'cuBLAS'
    elif 'gemm' in kernel_name.lower():
        info['operation_type'] = 'GEMM (Matrix-Matrix Multiplication)'  
        info['kernel_type'] = 'cuBLAS'
    elif 'sgemm' in kernel_name.lower():
        info['operation_type'] = 'SGEMM (Single Precision GEMM)'
        info['kernel_type'] = 'cuBLAS'
    elif 'hgemm' in kernel_name.lower():
        info['operation_type'] = 'HGEMM (Half Precision GEMM)'
        info['kernel_type'] = 'cuBLAS'
    
    # æ£€æµ‹æ•°æ®ç±»å‹
    if '__nv_bfloat16' in kernel_name:
        info['data_type'] = 'bfloat16'
    elif 'half' in kernel_name.lower():
        info['data_type'] = 'float16'
    elif 'float' in kernel_name.lower():
        info['data_type'] = 'float32'
    
    # æ£€æµ‹Flash Attention
    if 'flash' in kernel_name.lower() or 'fmha' in kernel_name.lower():
        info['operation_type'] = 'Flash Attention'
        info['kernel_type'] = 'Flash Attention'
    
    # æ£€æµ‹softmax
    if 'softmax' in kernel_name.lower():
        info['operation_type'] = 'Softmax'
        info['kernel_type'] = 'Custom'
    
    # æ£€æµ‹transpose
    if 'transpose' in kernel_name.lower():
        info['operation_type'] = 'Transpose'
        info['kernel_type'] = 'cuBLAS/Custom'
    
    return info

def map_pytorch_operation_to_kernel(pytorch_op: str) -> str:
    """
    å°†PyTorchæ“ä½œæ˜ å°„åˆ°é¢„æœŸçš„kernelç±»å‹
    """
    mapping = {
        'ATTENTION_QK_MATMUL': 'é€šå¸¸å¯¹åº”cuBLAS GEMM kernel (Q @ K^T)',
        'ATTENTION_WEIGHTS_VALUE_MATMUL': 'é€šå¸¸å¯¹åº”cuBLAS GEMV/GEMM kernel (Attention @ V)',
        'GEMM_QUERY_KEY_TRANSPOSE': 'cuBLAS GEMM kernel for Q@K^Tè®¡ç®—',
        'GEMM_ATTENTION_VALUES': 'cuBLAS GEMM/GEMV kernel for Attention@Vè®¡ç®—',
        'ATTENTION_SOFTMAX': 'Softmax kernel',
        'ATTENTION_LAYER_FORWARD': 'åŒ…å«å¤šä¸ªattentionç›¸å…³kernels',
        'MODEL_FORWARD_DECODE': 'åŒ…å«æ•´ä¸ªæ¨¡å‹å‰å‘ä¼ æ’­çš„kernels',
        'PREFILL_PHASE': 'é¢„å¡«å……é˜¶æ®µï¼Œé€šå¸¸åŒ…å«å¤§é‡å¹¶è¡Œè®¡ç®—',
        'DECODE_STEP': 'å•æ­¥è§£ç ï¼Œé€šå¸¸kernelè¾ƒå°'
    }
    
    for op_pattern, description in mapping.items():
        if op_pattern in pytorch_op:
            return description
    
    return 'æœªçŸ¥æ“ä½œ'

def create_kernel_analysis_report():
    """
    åˆ›å»ºkernelåˆ†ææŠ¥å‘Š
    """
    report = """
# CUDA Kernel ä¸ PyTorch æ“ä½œå¯¹åº”å…³ç³»åˆ†ææŒ‡å—

## å¸¸è§çš„Attentionè®¡ç®—ä¸­çš„Kernelç±»å‹

### 1. cuBLAS GEMV Kernels
**ç‰¹å¾**: `internal::gemvx::kernel`, `cublasGemvParamsEx`
**å¯¹åº”PyTorchæ“ä½œ**: 
- Query @ Key^T çŸ©é˜µä¹˜æ³•
- Attention_weights @ Value çŸ©é˜µä¹˜æ³•

**ç¤ºä¾‹kernelåç§°**:
```
std::enable_if<true, void>::type internal::gemvx::kernel<int, int, __nv_bfloat16, ...>
```

**å¦‚ä½•è¯†åˆ«**:
- åœ¨TensorBoardä¸­æŸ¥æ‰¾ `ATTENTION_QK_MATMUL_*` æˆ– `ATTENTION_WEIGHTS_VALUE_MATMUL_*` æ ‡è®°
- è¿™äº›æ ‡è®°åŒ…å«å½¢çŠ¶ä¿¡æ¯: B(batch), H(heads), S(sequence), D(dimension)

### 2. cuBLAS GEMM Kernels  
**ç‰¹å¾**: `sgemm`, `hgemm`, `gemm`
**å¯¹åº”PyTorchæ“ä½œ**: é€šç”¨çŸ©é˜µä¹˜æ³•

### 3. Flash Attention Kernels
**ç‰¹å¾**: `fmha`, `flash_attn`
**å¯¹åº”PyTorchæ“ä½œ**: ä¼˜åŒ–çš„attentionè®¡ç®—

### 4. Softmax Kernels
**ç‰¹å¾**: `softmax`
**å¯¹åº”PyTorchæ“ä½œ**: attentionæƒé‡å½’ä¸€åŒ–

## å¦‚ä½•åœ¨TensorBoardä¸­è¿½è¸ª

### 1. æŒ‰æ“ä½œç±»å‹è¿‡æ»¤
åœ¨TensorBoardçš„"TRACE"è§†å›¾ä¸­ï¼š
- æœç´¢ `ATTENTION_QK_MATMUL` æ‰¾åˆ°Q@K^Tè®¡ç®—
- æœç´¢ `ATTENTION_WEIGHTS_VALUE_MATMUL` æ‰¾åˆ°Attention@Vè®¡ç®—
- æœç´¢ `GEMM_` æ‰¾åˆ°å…·ä½“çš„GEMMæ“ä½œ

### 2. æŸ¥çœ‹è°ƒç”¨æ ˆ
- å±•å¼€traceä¸­çš„æ“ä½œ
- æŸ¥çœ‹"Call Stack"äº†è§£è°ƒç”¨å±‚æ¬¡
- åŒ¹é…è‡ªå®šä¹‰æ ‡è®°ä¸åº•å±‚kernel

### 3. åˆ†ææ€§èƒ½çƒ­ç‚¹
- æŒ‰æ—¶é—´æ’åºæ‰¾åˆ°æœ€è€—æ—¶çš„kernel
- å¯¹æ¯”ä¸åŒcontext lengthçš„æ€§èƒ½å·®å¼‚
- å…³æ³¨memory bandwidth vs compute utilization

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. GEMV vs GEMM
- çŸ­åºåˆ—(decodeé˜¶æ®µ): ä¸»è¦æ˜¯GEMVæ“ä½œ
- é•¿åºåˆ—(prefillé˜¶æ®µ): ä¸»è¦æ˜¯GEMMæ“ä½œ
- GEMVé€šå¸¸memory-bound, GEMMé€šå¸¸compute-bound

### 2. æ•°æ®ç±»å‹å½±å“
- bfloat16: å¹³è¡¡ç²¾åº¦å’Œæ€§èƒ½
- float16: æ›´å¿«ä½†å¯èƒ½æœ‰æ•°å€¼é—®é¢˜  
- float32: ç²¾åº¦æœ€é«˜ä½†æœ€æ…¢

### 3. æ‰¹å¤„ç†å¤§å°
- è¾ƒå¤§batch sizeæœ‰åˆ©äºGEMMæ€§èƒ½
- ä½†ä¼šå¢åŠ å†…å­˜å ç”¨

## å¸¸è§é—®é¢˜è¯Šæ–­

### Q: ä¸ºä»€ä¹ˆçœ‹ä¸åˆ°Flash Attention kernels?
A: å¯èƒ½å› ä¸º:
1. æ¨¡å‹ä½¿ç”¨äº†eager attentionå®ç°
2. åºåˆ—é•¿åº¦ä¸æ»¡è¶³Flash Attentionçš„è¦æ±‚
3. ç¡¬ä»¶ä¸æ”¯æŒ

### Q: ä¸ºä»€ä¹ˆGEMV kernelå ç”¨å¾ˆå¤šæ—¶é—´?
A: åœ¨decodeé˜¶æ®µå¾ˆæ­£å¸¸ï¼Œå› ä¸º:
1. Decodeæ˜¯é€tokenç”Ÿæˆï¼Œmatrix-vectoræ“ä½œå±…å¤š
2. Memory bandwidthæˆä¸ºç“¶é¢ˆ
3. å¹¶è¡Œåº¦ç›¸å¯¹è¾ƒä½

### Q: å¦‚ä½•åˆ¤æ–­æ€§èƒ½ç“¶é¢ˆ?
A: æŸ¥çœ‹:
1. Kernel occupancy (GPUåˆ©ç”¨ç‡)
2. Memory bandwidth utilization  
3. è®¡ç®—vså†…å­˜ä¼ è¾“æ¯”ä¾‹
"""
    
    return report

def analyze_profiling_directory(profile_dir: str):
    """
    åˆ†ææŒ‡å®šç›®å½•ä¸‹çš„profilingç»“æœ
    """
    print(f"\nåˆ†æç›®å½•: {profile_dir}")
    print("="*60)
    
    if not os.path.exists(profile_dir):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {profile_dir}")
        return
    
    # æŸ¥æ‰¾traceæ–‡ä»¶
    trace_files = list(Path(profile_dir).glob("*.pt.trace.json"))
    
    if not trace_files:
        print("âŒ æœªæ‰¾åˆ°traceæ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œprofiling")
        return
    
    print(f"âœ… æ‰¾åˆ° {len(trace_files)} ä¸ªtraceæ–‡ä»¶")
    
    # æ£€æŸ¥TensorBoardäº‹ä»¶æ–‡ä»¶
    tb_files = list(Path(profile_dir).glob("*.tensorboard.pt.trace.json"))
    if tb_files:
        print(f"âœ… TensorBoardæ–‡ä»¶å¯ç”¨: {len(tb_files)} ä¸ª")
        print(f"   è¿è¡Œå‘½ä»¤æŸ¥çœ‹: tensorboard --logdir={profile_dir}")
    
    print("\nğŸ“Š åˆ†æå»ºè®®:")
    print("1. åœ¨TensorBoardä¸­æŸ¥çœ‹ 'TRACE' è§†å›¾")
    print("2. æœç´¢ä»¥ä¸‹å…³é”®æ ‡è®°:")
    print("   - ATTENTION_QK_MATMUL_* (Q@K^Tè®¡ç®—)")
    print("   - ATTENTION_WEIGHTS_VALUE_MATMUL_* (Attention@Vè®¡ç®—)")
    print("   - DECODE_STEP_* (å•æ­¥tokenç”Ÿæˆ)")
    print("3. æŸ¥çœ‹kernelè¯¦ç»†ä¿¡æ¯å’Œè°ƒç”¨æ ˆ")
    print("4. å¯¹æ¯”ä¸åŒcontext lengthçš„æ€§èƒ½å·®å¼‚")

def main():
    """
    ä¸»å‡½æ•° - åˆ†ææ‰€æœ‰profilingç»“æœ
    """
    print("ğŸ” CUDA Kernel åˆ†æå·¥å…·")
    print("="*80)
    
    # åˆ›å»ºåˆ†ææŠ¥å‘Š
    report = create_kernel_analysis_report()
    
    # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
    report_file = "profiling_analysis_guide.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    print(f"âœ… åˆ†ææŒ‡å—å·²ä¿å­˜åˆ°: {report_file}")
    
    # åˆ†æç°æœ‰çš„profilingç›®å½•
    base_dir = "./profiling_logs"
    if os.path.exists(base_dir):
        print(f"\nğŸ“ æ£€æŸ¥profilingç»“æœç›®å½•: {base_dir}")
        
        # æŸ¥æ‰¾æ‰€æœ‰context lengthç›®å½•
        for item in os.listdir(base_dir):
            item_path = os.path.join(base_dir, item)
            if os.path.isdir(item_path) and item.startswith('ctx_len_'):
                analyze_profiling_directory(item_path)
    else:
        print(f"\nâŒ Profilingç›®å½•ä¸å­˜åœ¨: {base_dir}")
        print("è¯·å…ˆè¿è¡Œ mat_vec_mul_inference_profiling.py ç”Ÿæˆprofilingæ•°æ®")
    
    print("\n" + "="*80)
    print("ğŸ¯ å¿«é€Ÿå¼€å§‹æŒ‡å—:")
    print("1. è¿è¡Œprofiling: python mat_vec_mul_inference_profiling.py")
    print("2. å¯åŠ¨TensorBoard: tensorboard --logdir=./profiling_logs")
    print("3. åœ¨æµè§ˆå™¨æ‰“å¼€: http://localhost:6006")
    print("4. åˆ‡æ¢åˆ° 'TRACE' è§†å›¾æŸ¥çœ‹è¯¦ç»†çš„kernelè°ƒç”¨")
    print("5. ä½¿ç”¨æœç´¢åŠŸèƒ½æ‰¾åˆ°ç‰¹å®šçš„operationæ ‡è®°")
    print("="*80)

if __name__ == "__main__":
    main() 